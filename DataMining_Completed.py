# -*- coding: utf-8 -*-
"""project part 2 _ part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wh96pftgo6Jwho2lK9PUbDjMmNsozto8

# üöÄ Project Phase 2: Physical Design and Data Staging

Data staging is the process of preparing and organizing data for migration or integration into a data warehouse, data mart, or other analytical environments. In this case, we will be preparing our data for a data mart as intended for this project.


It also involves transforming and cleansing raw data from various sources into a format suitable for analysis and reporting.


This report will contain:

1. Source code (well documented code)

2. Submit a PDF file with the following details.

A. one-page schematic with your high-level data staging plan.  

B. Any other details you want to add.

C. A list of data quality issues you encountered and how you handled them (i.e., how did you detect and handle missing or noisy data (if any). How did you integrate the data from different sources etc.?

D. Fill out the attached excel sheet (Team Planning) and include it in the PDF

# ‚úèÔ∏è Project Preliminaries

Project Phase 2
Physical Design and Data Staging

Due Date: March 27, 2024, 11:59pm

Team #52

Student #1 : C√©leste Duguay #300122287

Student #2 : Michel Akotondji #300189827

Student #3: A√Øssatou K. Diome #300139476



---


Liens importants:


Github repository:https://github.com/lixu4n/DataMart-Design-And-Implementation

Project Team Planning XLS: https://uottawa-my.sharepoint.com/personal/cdugu093_uottawa_ca/Documents/Phase%202-Team%20Planning_W23_.xlsx?d=w470a72341981471e9f7f412236398574&csf=1&web=1&e=ngb0r1

# üõí Data Staging Steps ETL - Extraction

Data can be extracted in various formats, such as CSV, XML, or JSON.

For our project we will be extracting from 3 CSV formats.


1. CSV 1 : Consumer Behavior and Shopping Habits Dataset
https://www.kaggle.com/datasets/zeesolver/consumer-behavior-and-shopping-habits-dataset

2. CSV 2 : E-commerce Customer Data For Behavior Analysis
https://www.kaggle.com/datasets/shriyashjagtap/e-commerce-customer-for-behavior-analysis

3. CSV 3: E-commerce Customer Behvior Dataset
https://www.kaggle.com/datasets/uom190346a/e-commerce-customer-behavior-dataset
"""

# Import Libraries

import pandas as pd
import itertools
import numpy as np
import random
import math

from sklearn.preprocessing import MinMaxScaler

# Might be easier to use RAW Github Urls than load from google drive as it is better for access and collaboration.

# shopping_behavior_updated.csv (CSV1)
url1 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/shopping_behavior_updated.csv?token=GHSAT0AAAAAACMDFQODBU4JMXCDJLLZ5UGUZP3TXPA"
dataset1 = pd.read_csv(url1)

# ecommerce_customer_data_custom_ratios.csv (CSV2)
url2 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/ecommerce_customer_data_custom_ratios.csv?token=GHSAT0AAAAAACMDFQODV5DYK2SE5GYNWJMSZP3TUVQ"
dataset2 = pd.read_csv(url2)

# E-commerce Customer Behavior - Sheet1.csv (CSV3)
url3 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/E-commerce%20Customer%20Behavior%20-%20Sheet1.csv?token=GHSAT0AAAAAACMDFQOCVVCNTCFPCXQM2D6CZP3TVGQ"
dataset3 = pd.read_csv(url3)

# Display first rows  -

dataset1.head(10)

# Display first rows  - ecommerce_customer_data_custom_ratios.csv (CSV2)

dataset2.head(10)

# Display first rows  - ecommerce_customer_data_large.csv (CSV3

dataset3.head(10)

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset1.columns

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset2.columns

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset3.columns



# Datasets colums types

print(dataset1.dtypes)
print(dataset2.dtypes)
print(dataset3.dtypes)

"""# üõí Data Staging Steps ETL - Transformation - Data Cleaning

Handling duplicates, dropping unwanted columns and changing data types
"""

# b. Dropping Rows or Columns: Remove rows or columns
# with a high proportion of missing values if they cannot be reasonably imputed.

# Here we will perfrom a dropping a few colums in order to prepare to merge the datasets together.

dataset1 = dataset1.drop(columns=['Size', 'Color', 'Subscription Status', 'Shipping Type', 'Promo Code Used', 'Previous Purchases', 'Payment Method'])

print(dataset1.dtypes)

# Convert seasons to respective int value from 1-4

dataset1['Season'] = dataset1['Season'].replace('Winter', 1)
dataset1['Season'] = dataset1['Season'].replace('Spring', 2)
dataset1['Season'] = dataset1['Season'].replace('Summer', 3)
dataset1['Season'] = dataset1['Season'].replace('Fall', 4)

# Convert column season to int

dataset1['Season'] = dataset1['Season'].astype(int)
print(dataset1.dtypes)

dataset1.head(10)

dataset2 = dataset2.drop(columns=['Payment Method', 'Customer Age', 'Returns', 'Customer Name'])

print(dataset2.dtypes)
dataset2.head(10)

# Convert 'Purchase Date' to datetime
dataset2['Purchase Date'] = pd.to_datetime(dataset2['Purchase Date'], infer_datetime_format=True)


# Convert seasons to respective int value from 1-4 using the datetime format
dataset2['Season'] = np.where(dataset2['Purchase Date'].dt.month.isin([12, 1, 2]), 1,
                              np.where(dataset2['Purchase Date'].dt.month.isin([3, 4, 5]), 2,
                              np.where(dataset2['Purchase Date'].dt.month.isin([6, 7, 8]), 3,
                              np.where(dataset2['Purchase Date'].dt.month.isin([9, 10, 11]), 4, np.nan))))

# Convert column 'Season' to int
dataset2['Season'] = dataset2['Season'].astype(int)

# Convert 'Purchase Date' to just the date part after season calculation
dataset2['Purchase Date'] = dataset2['Purchase Date'].dt.date


print(dataset2.dtypes)
dataset2.head(10)

# dataset3
dataset3 = dataset3.drop(columns=['Membership Type', 'Days Since Last Purchase'])
print(dataset3.dtypes)
dataset3.head(10)

"""**Handling missing values**


  1. Handling missing values
  2. Scaling and Normlization!
  
Scaling down numeric variables to a range between 0 and 1, also known as normalization, has several benefits:

Uniformity: It brings all the numeric features within the same scale, ensuring that no single feature dominates due to its larger magnitude.

Interpretability: Scaling to a range between 0 and 1 preserves the relative differences in the data while making it easier to interpret. For instance, if Age ranges from 20 to 60 and Purchase Amount ranges from 100 to 1000, after scaling, you know that an increase of 0.1 in a feature represents an increase of 10% within the original range.
"""

#handle missing values
dataset1.fillna(method='ffill', inplace=True)  # forward fill imputation
dataset2.fillna(method='ffill', inplace=True)  # foward fill imputation
dataset1.fillna(method='ffill', inplace=True)  # forward fill imputation

"""Normalization

"""

# Normalize numeric val
scaler = MinMaxScaler()
dataset1[[ 'Review Rating']] = scaler.fit_transform(dataset1[[ 'Review Rating']])

dataset2[[ 'Quantity', 'Total Purchase Amount']] = scaler.fit_transform(dataset2[[ 'Quantity', 'Total Purchase Amount']])
dataset3[['Total Spend', 'Items Purchased', 'Average Rating']] = scaler.fit_transform(dataset3[['Total Spend', 'Items Purchased', 'Average Rating']])

"""# **Dataset Integration**"""

# Int√©gration des donn√©es sur la colonne Customer ID
merged_data = pd.merge(dataset1, dataset2, on="Customer ID", how="inner")
merged_data = pd.merge(merged_data, dataset3, on="Customer ID", how="inner")

print(merged_data.head(5))

"""# **DISCRETIZATION**"""

integrated_df = merged_data

# D√©finition des intervalles de prix
price_bins = [0, 30, 60, 90, float('inf')]
price_labels = ['Prix abordable', 'Prix moyen', 'Prix mod√©r√©', 'Prix √©lev√©']

# Discr√©tisation des donn√©es sur les prix
integrated_df['Price_Category'] = pd.cut(integrated_df['Purchase Amount (USD)'], bins=price_bins, labels=price_labels, right=False)

print("Integrated Dataset Price Category Value Counts:")
print(integrated_df['Price_Category'].value_counts())

# D√©finition des intervalles d'√¢ge
age_bins = [0, 18, 30, 50, float('inf')]
age_labels = ['Enfant', 'Jeune', 'Adulte', 'Vieux']

# Discr√©tisation des donn√©es sur l'√¢ge
integrated_df['Age_Category'] = pd.cut(integrated_df['Age'], bins=age_bins, labels=age_labels, right=False)

print("Integrated Dataset Age Category Value Counts:")
print(integrated_df['Age_Category'].value_counts())

"""# Loading"""

# G√©n√©rer des cl√©s substitu√©es pour customer id
customer_id_mapping = {customer_id: idx for idx, customer_id in enumerate(integrated_df['Customer ID'].unique(), start=1)}

# nouvelle colonne avec les cl√©s substitu√©es
integrated_df['Customer ID Substituted'] = integrated_df['Customer ID'].map(customer_id_mapping)

# DataFrame avec les cl√©s substitu√©es
integrated_df.to_csv("integrated_dataset_with_substituted_keys.csv", index=False)

# Chargement de l'ensemble de donn√©es final avec les cl√©s substitu√©es
final_dataset_with_substituted_keys = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

dataset_final = pd.read_csv("integrated_dataset_with_substituted_keys.csv")
print(dataset_final.head(3))

"""# Our data mart using a Database Management System (DBMS)"""

import sqlite3

# connexion √† la base de donn√©es (elle sera cr√©√©e)
conn = sqlite3.connect('spotify.db')

# curseur pour ex√©cuter des commandes SQL
cursor = conn.cursor()

# Chargement du dataset final avec les cl√©s substitu√©es √† partir du fichier CSV
integrated_df_with_keys = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

# Insertion des donn√©es dans la table du datamart
integrated_df_with_keys.to_sql('Datamart', conn, if_exists='replace', index=False)

# Validation des changements
conn.commit()

print("Le datamart a √©t√© cr√©√© avec succ√®s √† partir du dataset final.")

"""# üìä Project Phase 3 - OLAP + BI Dashboard

# Part 1 -  Standard OLAP Operations

a. Explore and collect

1. Exploring purchases by category and year:
"""

# Connexion √† la base de donn√©es
conn = sqlite3.connect('spotify.db')
cursor = conn.cursor()

# Ex√©cution de la requ√™te SQL
query = """
SELECT Category, strftime('%Y', "Date_d'achat") AS Year, COUNT(DISTINCT "Customer ID") AS Total_Clients
FROM Datamart
GROUP BY Category, Year;
"""
cursor.execute(query)


results1 = cursor.fetchall()
for row in results1:
    print(row)

"""2. Cumul des achats par mois"""

cursor = conn.cursor()

query = """
SELECT strftime('%m', "Date_d'achat") AS Month,
       SUM("Purchase Amount (USD)") AS Total_Achats
FROM Datamart
GROUP BY Month
ORDER BY Month;
"""
cursor.execute(query)


results2 = cursor.fetchall()
for row in results2:
    print(row)

# le r√©sultat indique un total de 104 373

"""b.Tranche, o√π une seule cote est s√©lectionn√©e
3. Nombre d'achats pour l'article 'Lunettes de soleil' :
"""

cursor = conn.cursor()

query = """
SELECT COUNT(*) AS Total_Achats_Lunettes_de_soleil
FROM Datamart
WHERE "Item Purchased" = 'Sunglasses';
"""
cursor.execute(query)


results3 = cursor.fetchall()
for row in results3:
    print(row)

"""C.D√©s, o√π l‚Äôon cr√©e un sous‚Äêcube
4. Sous-cube des achats par mois et cat√©gorie :
"""

cursor = conn.cursor()

query = """
SELECT strftime('%m', "Date_d'achat") AS Month, Category, COUNT(*) AS Total_Achats
FROM Datamart
GROUP BY Month, Category;

"""
cursor.execute(query)


results4 = cursor.fetchall()
for row in results4:
    print(row)

"""d. Combining OLAP operations.
1. Comparaison des achats entre deux p√©riodes :
"""

import sqlite3

# Connexion √† la base de donn√©es
conn = sqlite3.connect('spotify.db')
cursor = conn.cursor()

# Ex√©cution de la requ√™te PRAGMA
cursor.execute("PRAGMA table_info(Datamart)")

# R√©cup√©ration des r√©sultats
columns_info = cursor.fetchall()

# Affichage des noms de colonnes
column_names = [info[1] for info in columns_info]
print(column_names)

cursor = conn.cursor()

query = """
SELECT
    CASE
        WHEN strftime('%m', "Purchase Date") IN ('01','02','03') THEN 1
        WHEN strftime('%m', "Purchase Date") IN ('04','05','06') THEN 2
        WHEN strftime('%m', "Purchase Date") IN ('07','08','09') THEN 3
        ELSE 4
    END AS Season,
    strftime('%Y', "Purchase Date") AS Year,
    SUM("Purchase Amount (USD)") AS Total_Sales
FROM
    Datamart
WHERE
    strftime('%Y', "Purchase Date") IN ('2022', '2023')
GROUP BY
    Season, Year
ORDER BY
    Year, Season;
"""
cursor.execute(query)


results = cursor.fetchall()
results

"""2. Analyse des ventes par r√©gion et cat√©gorie"""

cursor = conn.cursor()

query = """
SELECT Location, Category, SUM("Purchase Amount (USD)") AS Total_Sales
FROM Datamart
GROUP BY Location, Category;
"""
cursor.execute(query)


results = cursor.fetchall()
results

"""## üìä Part 2 - Explorative Operations - 3 queries

a) Iceberg Querie
"""

# Iceberg Query
# In the first query, we will find the top 5 customers with the highest total spend over all purchases...
# we take our full csv file and execute it.

# we select our Customer ID and then calculate all the purchased with the associated id
# we will display by descending order
# we also only display the top 5!
iceberg_query = """
SELECT "Customer ID", SUM("Total Purchase Amount") AS total_spend
FROM Datamart
GROUP BY "Customer ID"
ORDER BY total_spend DESC
LIMIT 5;
"""

# My query -- This is using the Iceberg
cursor.execute(iceberg_query)

# get res
iceberg_results = cursor.fetchall()

# Print the results
print("Top 5 Customers with the Highest Total Spend of all purchases:")
for row in iceberg_results:
    print(row)

# close connection

conn.close()

"""b) Windowing Querie"""

# query : comparaison of each average purchase amount for each customer with the average puchase across all
#using thr windowing query
sql_query = """
SELECT "Customer ID",
       AVG("Total Purchase Amount") AS avg_purchase_amount,
       AVG("Total Purchase Amount") OVER () AS overall_avg_purchase_amount
FROM Datamart
GROUP BY "Customer ID";
"""

# Execution
cursor.execute(sql_query)

results = cursor.fetchall()

# Print
for row in results:
    print(row)

conn.close()

"""c. Using the window clause"""

# this takes the total spend and compare to location.. we will be using the window clause!
sql_query = """
SELECT "Customer ID",
       "Location",
       SUM("Total Purchase Amount") AS total_spend,
       AVG(SUM("Total Purchase Amount")) OVER (PARTITION BY "Location") AS avg_spend_per_location
FROM Datamart
GROUP BY "Customer ID", "Location";
"""

# Execution  query
cursor.execute(sql_query)

results = cursor.fetchall()

# Print
for row in results:
    print(row)

# cursor connection.
cursor.close()
conn.close()

"""In the phase 3 deliverable we also implemented a power bi dashboard to display our data.

# ‚õèÔ∏è Phase 4 - Data Mining

# PART A

# 1. R√©sum√© des donn√©es

**i) Nuages de points :**

Nous cr√©ons ici des nuages de points pour explorer les relations entre diff√©rentes variables, telles que l'√¢ge et le montant des achats et ensuite la note de r√©vision et le montant des achats. Cela nous permettra de visualiser s'il existe des tendances ou des corr√©lations entre ces variables.
"""

import matplotlib.pyplot as plt
import seaborn as sns

#les donn√©es
data = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

# Visualisation de la relation entre l'√¢ge et le montant des achats avec une ligne de r√©gression
plt.figure(figsize=(8, 6))
sns.regplot(data=data, x='Age', y='Purchase Amount (USD)', scatter_kws={'alpha':0.5})
plt.title('Relation entre l\'√¢ge et le montant des achats')
plt.xlabel('√Çge')
plt.ylabel('Montant des achats (USD)')
plt.show()

# Nuage de points pour explorer la relation entre la note de r√©vision et le montant des achats
plt.figure(figsize=(8, 6))
sns.regplot(data=data, x='Review Rating', y='Purchase Amount (USD)',scatter_kws={'alpha':0.5})
plt.title('Relation entre la note de r√©vision et le montant des achats')
plt.xlabel('Note de r√©vision')
plt.ylabel('Montant des achats (USD)')
plt.show()

"""**ii) Bo√Ætes √† moustaches :**

Nous utilisons des bo√Ætes √† moustaches pour visualiser la distribution des donn√©es num√©riques telles que l'√¢ge, le montant des achats et la note de r√©vision. Elles fourniront des informations sur la m√©diane, les quartiles et les valeurs aberrantes dans ces variables.
"""

# Bo√Æte √† moustaches pour visualiser la distribution de l'√¢ge
plt.figure(figsize=(8, 6))
sns.boxplot(data=data, y='Age')
plt.title('Distribution de l\'√¢ge')
plt.ylabel('√Çge')
plt.show()

# Bo√Æte √† moustaches pour visualiser la distribution du montant des achats
plt.figure(figsize=(8, 6))
sns.boxplot(data=data, y='Purchase Amount (USD)')
plt.title('Distribution du montant des achats')
plt.ylabel('Montant des achats (USD)')
plt.show()

# Calcule des statistiques pour l'√¢ge
age_median = np.median(data['Age'])
age_q1 = np.percentile(data['Age'], 25)
age_q3 = np.percentile(data['Age'], 75)
age_iqr = age_q3 - age_q1
age_lower_bound = age_q1 - 1.5 * age_iqr
age_upper_bound = age_q3 + 1.5 * age_iqr

print("Statistiques pour l'√¢ge:")
print("M√©diane:", age_median)
print("Premier quartile (Q1):", age_q1)
print("Troisi√®me quartile (Q3):", age_q3)
print("√âcart interquartile (IQR):", age_iqr)
print("Limite inf√©rieure (1.5 * IQR):", age_lower_bound)
print("Limite sup√©rieure (1.5 * IQR):", age_upper_bound)

# Calcule des statistiques pour le montant des achats
purchase_amount_median = np.median(data['Purchase Amount (USD)'])
purchase_amount_q1 = np.percentile(data['Purchase Amount (USD)'], 25)
purchase_amount_q3 = np.percentile(data['Purchase Amount (USD)'], 75)
purchase_amount_iqr = purchase_amount_q3 - purchase_amount_q1

print("\nStatistiques pour le montant des achats:")
print("M√©diane:", purchase_amount_median)
print("Premier quartile (Q1):", purchase_amount_q1)
print("Troisi√®me quartile (Q3):", purchase_amount_q3)
print("√âcart interquartile (IQR):", purchase_amount_iqr)

"""a) Pour l'√¢ge :

La m√©diane est de 32 ans, ce qui signifie que la moiti√© des individus ont un √¢ge inf√©rieur ou √©gal √† 32 ans et l'autre moiti√© ont un √¢ge sup√©rieur ou √©gal √† 32 ans.
Le premier quartile (Q1) est de 30 ans, ce qui signifie que 25% des individus ont un √¢ge inf√©rieur ou √©gal √† 30 ans.
Le troisi√®me quartile (Q3) est de 36 ans, ce qui signifie que 75% des individus ont un √¢ge inf√©rieur ou √©gal √† 36 ans.
L'√©cart interquartile (IQR) est de 6 ans.
Les limites inf√©rieure et sup√©rieure pour d√©tecter les valeurs aberrantes sont respectivement de 21 ans et 45 ans. Toute valeur d'√¢ge en dehors de ces limites peut √™tre consid√©r√©e comme une valeur aberrante.


b) Pour le montant des achats :

La m√©diane est de 61.0 USD, ce qui signifie que la moiti√© des montants d'achats sont inf√©rieurs ou √©gaux √† 61.0 USD et l'autre moiti√© sont sup√©rieurs ou √©gaux √† 61.0 USD.
Le premier quartile (Q1) est de 39.0 USD, ce qui signifie que 25% des montants d'achats sont inf√©rieurs ou √©gaux √† 39.0 USD.
Le troisi√®me quartile (Q3) est de 81.0 USD, ce qui signifie que 75% des montants d'achats sont inf√©rieurs ou √©gaux √† 81.0 USD.
L'√©cart interquartile (IQR) est de 42.0 USD.

**iii) Histogrammes :**

Nous utilisons les histogrammes pour explorer la distribution des donn√©es num√©riques telles que l'√¢ge, le montant des achats. Cela nous permettra de voir la r√©partition des valeurs et de d√©tecter des sch√©mas potentiels.
"""

# histogramme pour explorer la distribution de l'√¢ge
plt.figure(figsize=(8, 6))
plt.hist(data['Age'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution de l\'√¢ge')
plt.xlabel('√Çge')
plt.ylabel('Fr√©quence')
plt.grid(True)
plt.show()

# histogramme pour explorer la distribution du montant des achats
plt.figure(figsize=(8, 6))
plt.hist(data['Purchase Amount (USD)'], bins=30, color='lightgreen', edgecolor='black')
plt.title('Distribution du montant des achats')
plt.xlabel('Montant des achats (USD)')
plt.ylabel('Fr√©quence')
plt.grid(True)
plt.show()

"""# 2. Pr√©traitement des donn√©es

**i) Traitement des valeurs manquantes :**

Nous utiliserons des techniques d'imputation pour remplacer les valeurs manquantes par des valeurs estim√©es. Par exemple, nous allons remplacer les valeurs manquantes num√©riques par la moyenne ou la m√©diane des donn√©es existantes, et les valeurs cat√©gorielles par la modalit√© la plus fr√©quente.
"""

# Imputation des valeurs manquantes num√©riques avec la moyenne
numerical_columns = data.select_dtypes(include=['int', 'float']).columns
data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].mean())

# Imputation des valeurs manquantes cat√©gorielles avec la modalit√© la plus fr√©quente
categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    data[col] = data[col].fillna(data[col].mode()[0])

# V√©rification des valeurs manquantes apr√®s l'imputation
missing_values = data.isnull().sum()
print("Nombre de valeurs manquantes apr√®s imputation :\n", missing_values)

"""Toutes les valeurs manquantes ont √©t√© correctement trait√©es, car le nombre de valeurs manquantes pour chaque colonne est de 0. Cela signifie que nous avons r√©ussi √† remplacer les valeurs manquantes par des valeurs estim√©es, en utilisant la moyenne pour les colonnes num√©riques et la modalit√© la plus fr√©quente pour les colonnes cat√©gorielles. Nos donn√©es sont maintenant pr√™tes pour √™tre utilis√©es dans des analyses ult√©rieures ou dans des mod√®les d'apprentissage automatique.

**ii) Gestion des attributs cat√©goriels :**

Pour les variables cat√©gorielles, nous utiliserons l'encodage √† chaud (One-Hot Encoding) pour les transformer en variables binaires. Cela signifie que chaque cat√©gorie devient une colonne binaire (0 ou 1) dans le jeu de donn√©es.
Si les cat√©gories ont un ordre intrins√®que, nous allons √©galement les convertir en donn√©es ordinales. Par exemple, si on a des valeurs comme "faible", "moyen" et "√©lev√©", nous allons les encoder comme 0, 1 et 2 respectivement
"""

print("Premi√®res lignes des donn√©es :")
print(data.head())

# Encodage √† chaud des variables cat√©gorielles
data_encoded = pd.get_dummies(data, columns=['Gender_x', 'Category'])

print("\nDonn√©es apr√®s encodage √† chaud :")
print(data_encoded.head())

"""**iii) Normalisation des attributs num√©riques :**

Nous allons normaliser les attributs num√©riques pour mettre toutes les variables sur la m√™me √©chelle. Cela peut √™tre fait en soustrayant la moyenne et en divisant par l'√©cart-type (z-score normalization), ou en mettant √† l'√©chelle les valeurs dans une plage sp√©cifique (min-max scaling).
"""

# S√©lectionner les colonnes num√©riques √† normaliser
colonnes_numeriques = ['Age_x', 'Purchase Amount (USD)', 'Season_x', 'Review Rating', 'Total Spend', 'Items Purchased', 'Average Rating']

# Z-score normalization
data[colonnes_numeriques] = (data[colonnes_numeriques] - data[colonnes_numeriques].mean()) / data[colonnes_numeriques].std()

print("Donn√©es apr√®s normalisation z-score :")
print(data.head())

"""**iv) S√©lection d'entit√©s :**

Pour r√©duire la complexit√© du mod√®le et am√©liorer les performances, nous allons effectuer une s√©lection d'entit√©s en supprimant les attributs potentiellement redondants ou peu informatifs. Pour cela, nous allons utiliser des techniques comme l'analyse de corr√©lation, les m√©thodes bas√©es sur les arbres (comme l'importance des variables dans les for√™ts al√©atoires).

a) Analyse de corr√©lation :
"""

# S√©lectionner uniquement les colonnes num√©riques
data_numerique = data.select_dtypes(include=['int', 'float'])

# la matrice de corr√©lation pour les colonnes num√©riques
correlation_matrix = data_numerique.corr()

print("Matrice de corr√©lation :")
print(correlation_matrix)

""" b) Importance des variables dans les for√™ts al√©atoires :"""

from sklearn.ensemble import RandomForestClassifier

# S√©parer les variables explicatives et la variable cible
X = data_numerique.drop(columns=['Customer ID'])
y = data_numerique['Customer ID']

# mod√®le de for√™t al√©atoire
model = RandomForestClassifier()

# Adaptation du mod√®le aux donn√©es
model.fit(X, y)

#importance des variables
feature_importances = model.feature_importances_

# DataFrame pour visualiser l'importance des variables
feature_importances_df = pd.DataFrame({'Variable': X.columns, 'Importance': feature_importances})

# Trier les variables par importance
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)


print("Importance des variables :")
print(feature_importances_df)
plt.figure(figsize=(10, 6))
plt.barh(feature_importances_df['Variable'], feature_importances_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Variable')
plt.title('Importance des variables')
plt.show()

"""# PART B


Classification (Supervised Learning):


My task chosen here is determining if a customer will churn.
"""

# My imports from scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
import time

"""This preprocessing steps to ensure all categorical data is in the right format! All data to build a churn prediction model based on supervides learning."""

import pandas as pd

# Assuming 'data' is your DataFrame containing the dataset

# We drop data not needed for our analysis... why do customer churn? should not be based from purchase date item purchased etc.
data_numeric = data.drop(columns=['Customer ID', 'Item Purchased', 'Location', 'Purchase Date', 'City'])

# Discount could be a leading factor of customers NOT churning. lets process this data for our classifier.
if 'Discount Applied_x' in data_numeric.columns:
    data_numeric['Discount Applied_x'] = data_numeric['Discount Applied_x'].map({'Yes': 1, 'No': 0})

# Same for frequency, analysis how much it is bought in relations from customes behavior.
frequency_mapping = {'Daily': 1, 'Weekly': 2, 'Fortnightly': 3, 'Monthly': 4, 'Quarterly': 5, 'Bi-Weekly': 6}
data_numeric['Frequency of Purchases'] = data_numeric['Frequency of Purchases'].map(frequency_mapping)

"""1. Spliting the data into features such as X and Y

This step is crucial for our supervised learning model. We aim to used the input variables X and target features Y and train our model to know the relationships between the variables.

We wil be using the Churn input in this but it can but any datatypes: numerical, categorical and text etc.
"""

# perfoming encoding steps for our model and ensuring all data is dropped.
data_encoded = pd.get_dummies(data_numeric)
data_encoded = data_encoded.dropna()


# We split our data Churn into features (X) and target variable (y)

X = data_encoded.drop(columns=['Churn'])
y = data_encoded['Churn']

# We prepared our training model by splitting our training and testing sets. We opt to allocate 80% train and 20% test.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#test size can be changed here.

"""2. Result comparaison

In order fror us or clients to understand our model, we will plan accordingly by using dictionaries to store our results.

This will serve us during our analysis in step 3 as well of this part b.
"""

# we will use dictionaries to store our results
accuracy_dict = {}
precision_dict = {}
recall_dict = {}
time_dict = {}

"""3. Building our algorithms


We will use Decision Tree, Gradient Boosting and Random forest algorithms to construct models against our data.

This step is training the so call model....
"""

# Training steps

#Decision Tree Classifier
start_time = time.time()
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
end_time = time.time()
time_dict['Decision Tree'] = end_time - start_time

#Gradient Boosting classifier
start_time = time.time()
gb_classifier = GradientBoostingClassifier(random_state=42)
gb_classifier.fit(X_train, y_train)
end_time = time.time()
time_dict['Gradient Boosting'] = end_time - start_time

#Random forest classifier
#ensuring we get time for each .
start_time = time.time()
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)
end_time = time.time()
time_dict['Random Forest'] = end_time - start_time

"""Let's now test our models built. Return the values to perform an analysis."""

#Testing

# decision Tree Classifier
dt_predictions = dt_classifier.predict(X_test)
accuracy_dict['Decision Tree'] = accuracy_score(y_test, dt_predictions)
precision_dict['Decision Tree'] = precision_score(y_test, dt_predictions)
recall_dict['Decision Tree'] = recall_score(y_test, dt_predictions)

# Gradient Boosting Classifier
gb_predictions = gb_classifier.predict(X_test)
accuracy_dict['Gradient Boosting'] = accuracy_score(y_test, gb_predictions)
precision_dict['Gradient Boosting'] = precision_score(y_test, gb_predictions)
recall_dict['Gradient Boosting'] = recall_score(y_test, gb_predictions)

# Random Forest Classifier
rf_predictions = rf_classifier.predict(X_test)
accuracy_dict['Random Forest'] = accuracy_score(y_test, rf_predictions)
precision_dict['Random Forest'] = precision_score(y_test, rf_predictions)
recall_dict['Random Forest'] = recall_score(y_test, rf_predictions)

#Loop through our results, making our results accessible:

#1 - accuracy
print("Accuracy:")
for classifier, accuracy in accuracy_dict.items():
    print(f"{classifier}: {accuracy}")

#2 - Precision
print("\nPrecision:")
for classifier, precision in precision_dict.items():
    print(f"{classifier}: {precision}")

#3 - Recall

print("\nRecall:")
for classifier, recall in recall_dict.items():
    print(f"{classifier}: {recall}")

#4 - Time datatype.
print("\nTime to construct models:")
for classifier, time_taken in time_dict.items():
    print(f"{classifier}: {time_taken} seconds")

"""4. 300-400 word summary of our Classification Supervised Learning

Based on our results we explores many results from each algorithms asked in this task. For our first results, Accuruacy, We can see for most of the algorithms it returns a high rate of predicting churn. This suggests that all three models can be valuable in predicting customer churn based on the provided features. For our next results, precision Precision which mesures the accuracy of positive predictions made by the model. Both the Decision Tree and Random Forest models achieved perfect precision scores of 1.0, meaning they rarely misclassify non-churners as churners. Gradient Boosting performed slightly lower but still is at a percentage of 95.2%. This tells us that the models are reliable in identifying actual churn cases without many false positives.

Now, for our recall, also known as sensitivity, measures the ability of the model to correctly identify all positive instances that we stated in this analysis and model, including true positives and false negatives. The Decision Tree model achieved the highest recall score (92.16%), followed by Random Forest (90.20%), and then Gradient Boosting (78.43%) which is a great difference. This indicates that the Decision Tree model is more effective at capturing all actual churn instances.

 Lastly, we timed all our performances of the models to give us better insights in the runtime of each. This can help us differentiatr which is the best to used in real-life scenarios.The first model is the Decision Tree model which took 0.046 second to construct, while Gradient Boosting and Random Forest models took longer, with construction times of 0.989 and 1.057 seconds, respectively. Despite the longer construction times of Gradient Boosting and Random Forest, they still provided competitive accuracy and precision scores. Run time can be considered but does not determined much about their effectiveness of prediction.

# PART C

## Outliers Detection

Import Librairies
"""

from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler

"""## Preprocess and Feature Selection

Before applying the One-Class SVM, we need to select the numerical features for outlier detection and standardize them to have a mean of 0 and a standard deviation of 1. This normalization is crucial for algorithms like SVM that are sensitive to the scale of the data.

"""

df = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

# Select numerical features for outlier detection
features = df.select_dtypes(include=[np.number])

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

"""## Outlier Detection with One-Class SVM

With the data preprocessed, we apply the One-Class SVM to detect outliers. The "nu" parameter is set to control the fraction of outliers the algorithm will tolerate, and "gamma" defines the kernel coefficient for the RBF kernel.

"""

one_class_svm = OneClassSVM(kernel='rbf', nu=0.01, gamma='auto')
outlier_predictions = one_class_svm.fit_predict(features_scaled)

# Add a column to your dataframe to indicate outliers
df['is_outlier'] = outlier_predictions

"""## Display Outliers

Finally, we filter and display the rows identified as outliers by the One-Class SVM. These are the data points that differ significantly from the rest of the dataset according to the algorithm.

"""

# Filter the DataFrame to only include outliers
outliers_only = df[df['is_outlier'] == -1]

# Display dataset with only Outliers
outliers_only