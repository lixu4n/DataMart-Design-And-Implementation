# -*- coding: utf-8 -*-
"""project part 2 _ part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wh96pftgo6Jwho2lK9PUbDjMmNsozto8

# üöÄ Project Phase 2: Physical Design and Data Staging

Data staging is the process of preparing and organizing data for migration or integration into a data warehouse, data mart, or other analytical environments. In this case, we will be preparing our data for a data mart as intended for this project.


It also involves transforming and cleansing raw data from various sources into a format suitable for analysis and reporting.


This report will contain:

1. Source code (well documented code)

2. Submit a PDF file with the following details.

A. one-page schematic with your high-level data staging plan.  

B. Any other details you want to add.

C. A list of data quality issues you encountered and how you handled them (i.e., how did you detect and handle missing or noisy data (if any). How did you integrate the data from different sources etc.?

D. Fill out the attached excel sheet (Team Planning) and include it in the PDF

# ‚úèÔ∏è Project Preliminaries

Project Phase 2
Physical Design and Data Staging

Due Date: March 27, 2024, 11:59pm

Team #52

Student #1 : C√©leste Duguay #300122287

Student #2 : Michel Akotondji #300189827

Student #3: A√Øssatou K. Diome #300139476



---


Liens importants:


Github repository:https://github.com/lixu4n/DataMart-Design-And-Implementation

Project Team Planning XLS: https://uottawa-my.sharepoint.com/personal/cdugu093_uottawa_ca/Documents/Phase%202-Team%20Planning_W23_.xlsx?d=w470a72341981471e9f7f412236398574&csf=1&web=1&e=ngb0r1

# üõí Data Staging Steps ETL - Extraction

Data can be extracted in various formats, such as CSV, XML, or JSON.

For our project we will be extracting from 3 CSV formats.


1. CSV 1 : Consumer Behavior and Shopping Habits Dataset
https://www.kaggle.com/datasets/zeesolver/consumer-behavior-and-shopping-habits-dataset

2. CSV 2 : E-commerce Customer Data For Behavior Analysis
https://www.kaggle.com/datasets/shriyashjagtap/e-commerce-customer-for-behavior-analysis

3. CSV 3: E-commerce Customer Behvior Dataset
https://www.kaggle.com/datasets/uom190346a/e-commerce-customer-behavior-dataset
"""

# Import Libraries

import pandas as pd
import pandas as pd
import itertools
import numpy as np
import random
import math

from sklearn.preprocessing import MinMaxScaler

# Might be easier to use RAW Github Urls than load from google drive as it is better for access and collaboration.

# shopping_behavior_updated.csv (CSV1)
url1 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/shopping_behavior_updated.csv?token=GHSAT0AAAAAACMDFQODBU4JMXCDJLLZ5UGUZP3TXPA"
dataset1 = pd.read_csv(url1)

# ecommerce_customer_data_custom_ratios.csv (CSV2)
url2 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/ecommerce_customer_data_custom_ratios.csv?token=GHSAT0AAAAAACMDFQODV5DYK2SE5GYNWJMSZP3TUVQ"
dataset2 = pd.read_csv(url2)

# E-commerce Customer Behavior - Sheet1.csv (CSV3)
url3 = "https://raw.githubusercontent.com/lixu4n/DataMart-Design-And-Implementation/main/E-commerce%20Customer%20Behavior%20-%20Sheet1.csv?token=GHSAT0AAAAAACMDFQOCVVCNTCFPCXQM2D6CZP3TVGQ"
dataset3 = pd.read_csv(url3)

# Display first rows  -

dataset1.head(10)

# Display first rows  - ecommerce_customer_data_custom_ratios.csv (CSV2)

dataset2.head(10)

# Display first rows  - ecommerce_customer_data_large.csv (CSV3

dataset3.head(10)

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset1.columns

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset2.columns

# Get all colums display - Amazon Customer Behavior Survey.csv (CSV1)
dataset3.columns

# Datasets colums types

print(dataset1.dtypes)
print(dataset2.dtypes)
print(dataset3.dtypes)

"""# üõí Data Staging Steps ETL - Transformation - Data Cleaning

Handling duplicates, dropping unwanted columns and changing data types
"""

# b. Dropping Rows or Columns: Remove rows or columns
# with a high proportion of missing values if they cannot be reasonably imputed.

# Here we will perfrom a dropping a few colums in order to prepare to merge the datasets together.

dataset1 = dataset1.drop(columns=['Size', 'Color', 'Subscription Status', 'Shipping Type', 'Promo Code Used', 'Previous Purchases', 'Payment Method'])

print(dataset1.dtypes)

# Convert seasons to respective int value from 1-4

dataset1['Season'] = dataset1['Season'].replace('Winter', 1)
dataset1['Season'] = dataset1['Season'].replace('Spring', 2)
dataset1['Season'] = dataset1['Season'].replace('Summer', 3)
dataset1['Season'] = dataset1['Season'].replace('Fall', 4)

# Convert column season to int

dataset1['Season'] = dataset1['Season'].astype(int)
print(dataset1.dtypes)

dataset1.head(10)

dataset2 = dataset2.drop(columns=['Payment Method', 'Customer Age', 'Returns', 'Customer Name'])

print(dataset2.dtypes)
dataset2.head(10)

# Convert 'Purchase Date' to datetime
dataset2['Purchase Date'] = pd.to_datetime(dataset2['Purchase Date'], infer_datetime_format=True)


# Convert seasons to respective int value from 1-4 using the datetime format
dataset2['Season'] = np.where(dataset2['Purchase Date'].dt.month.isin([12, 1, 2]), 1,
                              np.where(dataset2['Purchase Date'].dt.month.isin([3, 4, 5]), 2,
                              np.where(dataset2['Purchase Date'].dt.month.isin([6, 7, 8]), 3,
                              np.where(dataset2['Purchase Date'].dt.month.isin([9, 10, 11]), 4, np.nan))))

# Convert column 'Season' to int
dataset2['Season'] = dataset2['Season'].astype(int)

# Convert 'Purchase Date' to just the date part after season calculation
dataset2['Purchase Date'] = dataset2['Purchase Date'].dt.date


print(dataset2.dtypes)
dataset2.head(10)

# dataset3
dataset3 = dataset3.drop(columns=['Membership Type', 'Days Since Last Purchase'])
print(dataset3.dtypes)
dataset3.head(10)

"""**Handling missing values**


  1. Handling missing values
  2. Scaling and Normlization!
  
Scaling down numeric variables to a range between 0 and 1, also known as normalization, has several benefits:

Uniformity: It brings all the numeric features within the same scale, ensuring that no single feature dominates due to its larger magnitude.

Interpretability: Scaling to a range between 0 and 1 preserves the relative differences in the data while making it easier to interpret. For instance, if Age ranges from 20 to 60 and Purchase Amount ranges from 100 to 1000, after scaling, you know that an increase of 0.1 in a feature represents an increase of 10% within the original range.
"""

#handle missing values
dataset1.fillna(method='ffill', inplace=True)  # forward fill imputation
dataset2.fillna(method='ffill', inplace=True)  # foward fill imputation
dataset1.fillna(method='ffill', inplace=True)  # forward fill imputation

"""Normalization

"""

# Normalize numeric val
scaler = MinMaxScaler()
dataset1[[ 'Review Rating']] = scaler.fit_transform(dataset1[[ 'Review Rating']])

dataset2[[ 'Quantity', 'Total Purchase Amount']] = scaler.fit_transform(dataset2[[ 'Quantity', 'Total Purchase Amount']])
dataset3[['Total Spend', 'Items Purchased', 'Average Rating']] = scaler.fit_transform(dataset3[['Total Spend', 'Items Purchased', 'Average Rating']])

"""# **Dataset Integration**"""

# Int√©gration des donn√©es sur la colonne Customer ID
merged_data = pd.merge(dataset1, dataset2, on="Customer ID", how="inner")
merged_data = pd.merge(merged_data, dataset3, on="Customer ID", how="inner")

print(merged_data.head(5))

"""# **DISCRETIZATION**"""

integrated_df = merged_data

# D√©finition des intervalles de prix
price_bins = [0, 30, 60, 90, float('inf')]
price_labels = ['Prix abordable', 'Prix moyen', 'Prix mod√©r√©', 'Prix √©lev√©']

# Discr√©tisation des donn√©es sur les prix
integrated_df['Price_Category'] = pd.cut(integrated_df['Purchase Amount (USD)'], bins=price_bins, labels=price_labels, right=False)

print("Integrated Dataset Price Category Value Counts:")
print(integrated_df['Price_Category'].value_counts())

# D√©finition des intervalles d'√¢ge
age_bins = [0, 18, 30, 50, float('inf')]
age_labels = ['Enfant', 'Jeune', 'Adulte', 'Vieux']

# Discr√©tisation des donn√©es sur l'√¢ge
integrated_df['Age_Category'] = pd.cut(integrated_df['Age'], bins=age_bins, labels=age_labels, right=False)

print("Integrated Dataset Age Category Value Counts:")
print(integrated_df['Age_Category'].value_counts())

"""# Loading"""

# G√©n√©rer des cl√©s substitu√©es pour customer id
customer_id_mapping = {customer_id: idx for idx, customer_id in enumerate(integrated_df['Customer ID'].unique(), start=1)}

# nouvelle colonne avec les cl√©s substitu√©es
integrated_df['Customer ID Substituted'] = integrated_df['Customer ID'].map(customer_id_mapping)

# DataFrame avec les cl√©s substitu√©es
integrated_df.to_csv("integrated_dataset_with_substituted_keys.csv", index=False)

# Chargement de l'ensemble de donn√©es final avec les cl√©s substitu√©es
final_dataset_with_substituted_keys = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

dataset_final = pd.read_csv("integrated_dataset_with_substituted_keys.csv")
print(dataset_final.head(3))

"""# Our data mart using a Database Management System (DBMS)"""

import sqlite3

# connexion √† la base de donn√©es (elle sera cr√©√©e)
conn = sqlite3.connect('spotify.db')

# curseur pour ex√©cuter des commandes SQL
cursor = conn.cursor()

# Chargement du dataset final avec les cl√©s substitu√©es √† partir du fichier CSV
integrated_df_with_keys = pd.read_csv("integrated_dataset_with_substituted_keys.csv")

# Insertion des donn√©es dans la table du datamart
integrated_df_with_keys.to_sql('Datamart', conn, if_exists='replace', index=False)

# Validation des changements
conn.commit()

print("Le datamart a √©t√© cr√©√© avec succ√®s √† partir du dataset final.")

"""# üìä Project Phase 3 - OLAP + BI Dashboard

# Part 1 -  Standard OLAP Operations

a. Explore and collect

1. Exploring purchases by category and year:
"""

# Connexion √† la base de donn√©es
conn = sqlite3.connect('spotify.db')
cursor = conn.cursor()

# Ex√©cution de la requ√™te SQL
query = """
SELECT Category, strftime('%Y', "Date_d'achat") AS Year, COUNT(DISTINCT "Customer ID") AS Total_Clients
FROM Datamart
GROUP BY Category, Year;
"""
cursor.execute(query)


results1 = cursor.fetchall()
for row in results1:
    print(row)

"""2. Cumul des achats par mois"""

cursor = conn.cursor()

query = """
SELECT strftime('%m', "Date_d'achat") AS Month,
       SUM("Purchase Amount (USD)") AS Total_Achats
FROM Datamart
GROUP BY Month
ORDER BY Month;
"""
cursor.execute(query)


results2 = cursor.fetchall()
for row in results2:
    print(row)

# le r√©sultat indique un total de 104 373

"""b.Tranche, o√π une seule cote est s√©lectionn√©e
3. Nombre d'achats pour l'article 'Lunettes de soleil' :
"""

cursor = conn.cursor()

query = """
SELECT COUNT(*) AS Total_Achats_Lunettes_de_soleil
FROM Datamart
WHERE "Item Purchased" = 'Sunglasses';
"""
cursor.execute(query)


results3 = cursor.fetchall()
for row in results3:
    print(row)

"""C.D√©s, o√π l‚Äôon cr√©e un sous‚Äêcube
4. Sous-cube des achats par mois et cat√©gorie :
"""

cursor = conn.cursor()

query = """
SELECT strftime('%m', "Date_d'achat") AS Month, Category, COUNT(*) AS Total_Achats
FROM Datamart
GROUP BY Month, Category;

"""
cursor.execute(query)


results4 = cursor.fetchall()
for row in results4:
    print(row)

"""d. Combining OLAP operations.
1. Comparaison des achats entre deux p√©riodes :
"""

import sqlite3

# Connexion √† la base de donn√©es
conn = sqlite3.connect('spotify.db')
cursor = conn.cursor()

# Ex√©cution de la requ√™te PRAGMA
cursor.execute("PRAGMA table_info(Datamart)")

# R√©cup√©ration des r√©sultats
columns_info = cursor.fetchall()

# Affichage des noms de colonnes
column_names = [info[1] for info in columns_info]
print(column_names)

cursor = conn.cursor()

query = """
SELECT
    CASE
        WHEN strftime('%m', "Purchase Date") IN ('01','02','03') THEN 1
        WHEN strftime('%m', "Purchase Date") IN ('04','05','06') THEN 2
        WHEN strftime('%m', "Purchase Date") IN ('07','08','09') THEN 3
        ELSE 4
    END AS Season,
    strftime('%Y', "Purchase Date") AS Year,
    SUM("Purchase Amount (USD)") AS Total_Sales
FROM
    Datamart
WHERE
    strftime('%Y', "Purchase Date") IN ('2022', '2023')
GROUP BY
    Season, Year
ORDER BY
    Year, Season;
"""
cursor.execute(query)


results = cursor.fetchall()
results

"""2. Analyse des ventes par r√©gion et cat√©gorie"""

cursor = conn.cursor()

query = """
SELECT Location, Category, SUM("Purchase Amount (USD)") AS Total_Sales
FROM Datamart
GROUP BY Location, Category;
"""
cursor.execute(query)


results = cursor.fetchall()
results

"""## üìä Part 2 - Explorative Operations - 3 queries

a) Iceberg Querie
"""

# Iceberg Query
# In the first query, we will find the top 5 customers with the highest total spend over all purchases...
# we take our full csv file and execute it.

# we select our Customer ID and then calculate all the purchased with the associated id
# we will display by descending order
# we also only display the top 5!
iceberg_query = """
SELECT "Customer ID", SUM("Total Purchase Amount") AS total_spend
FROM Datamart
GROUP BY "Customer ID"
ORDER BY total_spend DESC
LIMIT 5;
"""

# My query -- This is using the Iceberg
cursor.execute(iceberg_query)

# get res
iceberg_results = cursor.fetchall()

# Print the results
print("Top 5 Customers with the Highest Total Spend of all purchases:")
for row in iceberg_results:
    print(row)

# close connection
cursor.close()
conn.close()

"""b) Windowing Querie"""

# query : comparaison of each average purchase amount for each customer with the average puchase across all
#using thr windowing query
sql_query = """
SELECT "Customer ID",
       AVG("Total Purchase Amount") AS avg_purchase_amount,
       AVG("Total Purchase Amount") OVER () AS overall_avg_purchase_amount
FROM Datamart
GROUP BY "Customer ID";
"""

# Execution
cursor.execute(sql_query)

results = cursor.fetchall()

# Print
for row in results:
    print(row)

cursor.close()
conn.close()

"""c. Using the window clause"""

# this takes the total spend and compare to location.. we will be using the window clause!
sql_query = """
SELECT "Customer ID",
       "Location",
       SUM("Total Purchase Amount") AS total_spend,
       AVG(SUM("Total Purchase Amount")) OVER (PARTITION BY "Location") AS avg_spend_per_location
FROM Datamart
GROUP BY "Customer ID", "Location";
"""

# Execution  query
cursor.execute(sql_query)

results = cursor.fetchall()

# Print
for row in results:
    print(row)

# cursor connection.
cursor.close()
conn.close()

"""In the phase 3 deliverable we also implemented a power bi dashboard to display our data.

# ‚õèÔ∏è Phase 4 - Data Mining

To complete next phase
"""